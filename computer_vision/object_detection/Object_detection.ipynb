{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Key architectures of object detection models\n",
    "\n",
    "## 1. Classic algorithms\n",
    "### Viola-Jones Detector (2001) (Haar Cascade classifier)\n",
    "1. Classify images under the sliding window\n",
    "2. Mine features:\n",
    "<img src=\"1_images/haar_features.jpg\" width=\"800\">\n",
    "\n",
    "(sum of black - sum of white)\n",
    "We have ~16000 features for 24*24 image\n",
    "4. Use \"integral image\" to compute features efficiently.\n",
    "5. Use AdaBoost of weak classifiers (1-2 layered trees).\n",
    "\n",
    "<img src=\"1_images/haar_classifier.jpeg\" width=\"800\">\n",
    "\n",
    "### HOG Detector (2006)\n",
    "Mine features. Train SVM to classify in sliding fashion.\n",
    "### DPM (2008)\n",
    "Uses HOG features (or other).\n",
    "1. A coarse root filter defines a detection window that approximately covers an entire object. A filter specifies weights for a region feature vector..\n",
    "2. Multiple part filters that cover smaller parts of the object. Parts filters are learned at twice resolution of the root filter.\n",
    "3. A spatial model for scoring the locations of part filters relative to the root.\n",
    "\n",
    "Features:\n",
    "<img src=\"1_images/dpm_features.png\" width=\"1000\">\n",
    "\n",
    "Model:\n",
    "<img src=\"1_images/dpm_model.png\" width=\"1000\">\n",
    "\n",
    "### Overfeat (2013)\n",
    "1. Train CNN for classification in sliding window fashion.\n",
    "2. Replace the last classification layer with a regression layer (4 coordinates) and train it.\n",
    "3. Use these networks to detect objects and merge highly overlapped regions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. stages NNs:\n",
    "### RCNN (2014)\n",
    "1. Selective search algorithm to mine candidates.\n",
    "Selective search selects regions of similar color. Candidates are these regions + their unions.\n",
    "2. Classify them using CNN.\n",
    "~2000 candidates per image results in 47 seconds to classify 1 image.\n",
    "<img src=\"1_images/rcnn_model.jpg\" width=\"800\">\n",
    "\n",
    "### ZF-Net\n",
    "Based on RCNN.\n",
    "1. Selective Search.\n",
    "2. Feed image into CNN.\n",
    "3. Map regions found by Selective Search to further layers.\n",
    "This speeds up the solution a lot.\n",
    "\n",
    "### SPPNet (2014)\n",
    "Spatial Pyramid Pooling. Based on ZF-Net.\n",
    "Spatial Pyramid Pooling layer - pooling with window size and stride proportional to the input size. Allows to achieve fixed size output.\n",
    "\n",
    "### Fast RCNN (2015)\n",
    "Based on the SPPNet.\n",
    "1. Run image through pre-trained CNN\n",
    "2. Run selective search on the image to generate region proposals\n",
    "3. These proposals are then projected onto the feature maps, just like they are in SPPNet.\n",
    "4. Sampling strategy is applied for proposals\n",
    "5. For each region proposal in the feature maps, an ROI Pool layer computes a fix length vector. That vector goes through a couple of fully connected layers, which then split into two sibling branches.\n",
    "6. One branch to predict classes\n",
    "7. And the other to predict bounding boxes.\n",
    "8. Finally, we do NMS to reduce duplicate boxes.\n",
    "\n",
    "<p style=\"background-color: white;\">\n",
    "<img src=\"1_images/fastrcnn_model.png\" width=\"1000\">\n",
    "</p>\n",
    "\n",
    "### Faster RCNN (2015)\n",
    "1. CNN\n",
    "2. Region Proposal Network\n",
    "2.1 Put regular grid on the last feature map\n",
    "2.2 Extract anchors of different scale and aspect ration for each grid cell\n",
    "2.3 Put extracted regions into network predicting classes and bounding boxes relatively to the each anchor (implemented by convolutional layer)\n",
    "2.4 Apply Non Maximum Suppression\n",
    "3. Finetune class and bbox prediction for each candidate from RPN\n",
    "\n",
    "Architecture:\n",
    "<p style=\"background-color: white;\">\n",
    "<img src=\"1_images/faster_rcnn_model.png\" width=\"1000\">\n",
    "</p>\n",
    "\n",
    "### Mask R-CNN (2017)\n",
    "Faster RCNN + additional output for segmentation. Segmentation could be enhanced by the PointRend algorithm (more precise up-scaling of segmentation to the original size)\n",
    "\n",
    "### Feature Pyramid Networks (FPN) (2017)\n",
    "Save feature maps at different stages of backbone CNN evaluation, upsample further feature maps and combine them with previous ones. This gives us semantic reach feature maps in high resolution to detect objects precisely. FPN itself is just a feature extractor.\n",
    "For object detection separate layers for classification and bounding box coordinates prediction are applied to each feature map in a sliding window fashion.\n",
    "Could be used in conjunction with Faster RCNN architecture. Region Proposal Network predicts ROI. Based on the size of the ROI, we select the feature map layer in the most proper scale to extract the feature patches.\n",
    "<img src=\"1_images/fpn_model.png\" width=\"800\">\n",
    "\n",
    "### G-RCNN (2021)\n",
    "Key feature - application to the video stream. RPN considers spacial (colour) and temporal (from the frame sequence) information, combines it and extracts ROIs using CNN from these features. Then, classification and bounding-box regression is applied.\n",
    "<img src=\"1_images/g-rcnn_model.png\" width=\"800\">"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 1 stage NNs:\n",
    "### YOLOv1 (2016)\n",
    "CNN pre-trained on ImageNet. The last layers are replaced by extra convolutions and FCLs. Output 7*7*30 tensor which represents 98 bounding boxes, each pair belongs to one of 20 classes (49 class predictions). Lower accuracy compared to 2-stage detectors, but real-time level speed.\n",
    "### SSD (2016)\n",
    "Accuracy on the level of 2-stage detectors. Real-time speed. Architecture is very similar to Faster R-CNN but uses several feature maps and for each feature map cell outputs (C+4)*K numbers (C - number of classes, 4 - bounding box coordinates, K - anchors).\n",
    "### RetinaNet (2017)\n",
    "### YOLOv2,v3 (2017, 2018) -\n",
    "Different pre-train strategy (classification with low resolution -> classification with high resolution -> object detection with high resolution).\n",
    "Dropout is replaced by the Batch Normalization.\n",
    "Anchors are used and they are learned using K-Means.\n",
    "Fine-grained information for the last feature map (features from previous layers).\n",
    "Different classes for each anchor.\n",
    "Softmax was replaced by logistic classifiers (1 object could belong to several classes now).\n",
    "Box parametrization is more stable\n",
    "Boxes are predicted by convolutions instead of linear layers.\n",
    "Training using different scale images (the network is fully convolutional, so it is possible).\n",
    "A backbone network is replaced by custom DarkNet architecture.\n",
    "### YOLOv4 (2020)\n",
    "### YOLOv5 (2020)\n",
    "### YOLOv6\n",
    "### YOLOv7\n",
    "### YOLOR (2021)\n",
    "### YOLOv8 (2023)\n",
    "Modifications in convolutions\n",
    "Anchor-free prediction (or these boxes are optional?)\n",
    "Segmentation and classification capabilities\n",
    "Performance improvements\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
